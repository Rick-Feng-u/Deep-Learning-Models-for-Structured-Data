{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rick-Feng-u/Deep-Learning-Models-for-Structured-Data/blob/main/Training_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0x7PZn33HeE"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV6YKrZ_OzTN",
        "outputId": "ae04e132-58a1-4ff9-d7ac-3bc913954f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkMFNrImQuXT"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Honour_Thesis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hF8j3G5Q1zk"
      },
      "outputs": [],
      "source": [
        "from Util import sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N45WFoHYcbNM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKfdRLXPdi5Z"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Oh1ZjodoV9"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G5RKNLSj3Tp"
      },
      "outputs": [],
      "source": [
        "class attention_decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.1):\n",
        "        super(attention_decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2spU2D_Adp0U"
      },
      "outputs": [],
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length):\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for input_elem in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[input_elem], encoder_hidden)\n",
        "        encoder_outputs[input_elem] = encoder_output[0, 0]\n",
        "\n",
        "\n",
        "    decoder_input = torch.tensor([[0]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    for target in range(target_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "        loss += criterion(decoder_output, target_tensor[target])\n",
        "        decoder_input = target_tensor[target]\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K7dpEpddtrP"
      },
      "outputs": [],
      "source": [
        "def show_plot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5gelMCvd04d"
      },
      "outputs": [],
      "source": [
        "def train_iters(training_pairs, max_length, encoder, decoder, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    training_data_size = len(training_pairs)\n",
        "\n",
        "    for i in range(training_data_size):\n",
        "        training_pair = training_pairs[i][0]\n",
        "        input_idx_seq = training_pair[0]\n",
        "        target_idx_seq = training_pair[1]\n",
        "\n",
        "        input_idx_seq.append(1)  # EOS\n",
        "        target_idx_seq.append(1)\n",
        "\n",
        "        # print(input_idx_seq)\n",
        "        input_tensor = torch.tensor(input_idx_seq, dtype=torch.long, device=device).view(-1, 1)\n",
        "        target_tensor = torch.tensor(target_idx_seq, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(\"loss average  \" + str(print_loss_avg))\n",
        "\n",
        "        if i % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    show_plot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBXmY6TmRC7M"
      },
      "source": [
        "# **Wiki pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-FJDCpDeBTX"
      },
      "outputs": [],
      "source": [
        "train_pairs =[]\n",
        "\n",
        "with open('wiki_seq_1.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  train_pairs.extend(pair)\n",
        "\n",
        "with open('wiki_seq_2.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  train_pairs.extend(pair)\n",
        "\n",
        "with open('wiki_seq_3.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  train_pairs.extend(pair)\n",
        "\n",
        "with open('wiki_seq_4.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  train_pairs.extend(pair)\n",
        "\n",
        "with open('wiki_seq_5.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  train_pairs.extend(pair)\n",
        "\n",
        "with open('wiki_seq_6.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  train_pairs.extend(pair)\n",
        "\n",
        "with open('wiki_seq_9.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  train_pairs.extend(pair)\n",
        "\n",
        "with open('wiki_input_seq_class.pkl', 'rb') as f:\n",
        "  input_seq_class = pickle.load(f)\n",
        "\n",
        "with open('wiki_output_seq_class.pkl', 'rb') as f:\n",
        "  out_seq_class = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxNIcli5dyWb"
      },
      "outputs": [],
      "source": [
        "print(train_pairs[1000][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cj6t66tstfp"
      },
      "outputs": [],
      "source": [
        "list_len = [len(i[0]) for i in train_pairs]\n",
        "train_max= max(list_len) + 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDCuR_NUG-wO"
      },
      "outputs": [],
      "source": [
        "test_pairs = []\n",
        "\n",
        "with open('wiki_seq_7.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  test_pairs.extend(pair)\n",
        "\n",
        "with open('wiki_seq_8.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  test_pairs.extend(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSASDDKvG_Xm"
      },
      "outputs": [],
      "source": [
        "list_len = [len(i[0]) for i in test_pairs]\n",
        "test_max = max(list_len) + 2\n",
        "input_seq_class.highest_length = max(train_max, test_max)\n",
        "print(input_seq_class.highest_length)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_ = torch.load('encoder.pth')\n",
        "decoder_ = torch.load('decoder.pth')"
      ],
      "metadata": {
        "id": "2jt5m14AG8ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGxooH-hRJoY"
      },
      "source": [
        "# **DBLP pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_JafYO3RMMD"
      },
      "outputs": [],
      "source": [
        "pairs =[]\n",
        "\n",
        "with open('dblp_seq_1.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  pairs.extend(pair)\n",
        "\n",
        "with open('dblp_seq_2.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  pairs.extend(pair)\n",
        "\n",
        "with open('dblp_seq_3.pkl', 'rb') as f:\n",
        "  pair = pickle.load(f)\n",
        "  pairs.extend(pair)\n",
        "\n",
        "with open('dblp_input_seq_class.pkl', 'rb') as f:\n",
        "  input_seq_class = pickle.load(f)\n",
        "\n",
        "with open('dblp_output_seq_class.pkl', 'rb') as f:\n",
        "  out_seq_class = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gti_LFPSTtA"
      },
      "outputs": [],
      "source": [
        "print(out_seq_class.element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xigymUIwRhDK"
      },
      "outputs": [],
      "source": [
        "article = 0\n",
        "book = 0\n",
        "processing = 0\n",
        "inprocessding = 0\n",
        "www= 0\n",
        "master = 0\n",
        "colloc = 0\n",
        "phd = 0\n",
        "for pair in pairs:\n",
        "  if pair[0][1][0] == 2:\n",
        "    article += 1\n",
        "  elif pair[0][1][0] == 3:\n",
        "    book += 1\n",
        "  elif pair[0][1][0] == 4:\n",
        "    processing += 1\n",
        "  elif pair[0][1][0] == 5:\n",
        "    inprocessding += 1\n",
        "  elif pair[0][1][0] == 6:\n",
        "    www += 1\n",
        "  elif pair[0][1][0] == 7:\n",
        "    master += 1\n",
        "  elif pair[0][1][0] == 8:\n",
        "    colloc += 1\n",
        "  elif pair[0][1][0] == 9:\n",
        "    phd += 1\n",
        "\n",
        "print(\"a: \" +str(article) + \" b: \"+str(book)+\" p: \"+str(processing)+\" i: \"+str(inprocessding)+\" w: \"+str(www)+\" m: \"+str(master)+\" c: \"+str(colloc)+\" phd: \"+str(phd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxKiz0iWWCBt"
      },
      "outputs": [],
      "source": [
        "test_pairs = []\n",
        "num_art = 2000\n",
        "num_book = 800\n",
        "num_pro = 500\n",
        "num_inpro = 2000\n",
        "num_www = 800\n",
        "num_inco = 500\n",
        "num_phd = 500\n",
        "for pair in pairs:\n",
        "  if pair[0][1][0] == 2:\n",
        "    if num_art != 0:\n",
        "      test_pairs.append(pair)\n",
        "      pairs.remove(pair)\n",
        "      num_art -= 1\n",
        "  elif pair[0][1][0] == 3: \n",
        "    if num_book != 0:\n",
        "      test_pairs.append(pair)\n",
        "      pairs.remove(pair)\n",
        "      num_book -= 1\n",
        "  elif pair[0][1][0] == 4:\n",
        "    if num_pro != 0:\n",
        "      test_pairs.append(pair)\n",
        "      pairs.remove(pair)\n",
        "      num_pro -= 1\n",
        "  elif pair[0][1][0] == 5:\n",
        "    if num_inpro != 0:\n",
        "      test_pairs.append(pair)\n",
        "      pairs.remove(pair)\n",
        "      num_inpro -= 1\n",
        "  elif pair[0][1][0] == 6:\n",
        "    if num_www != 0:\n",
        "      test_pairs.append(pair)\n",
        "      pairs.remove(pair)\n",
        "      num_www -= 1\n",
        "  elif pair[0][1][0] == 8:\n",
        "    if num_inco != 0:\n",
        "      test_pairs.append(pair)\n",
        "      pairs.remove(pair)\n",
        "      num_inco -= 1\n",
        "  elif pair[0][1][0] == 9:\n",
        "    if num_phd != 0:\n",
        "      test_pairs.append(pair)\n",
        "      pairs.remove(pair)\n",
        "      num_phd -= 1\n",
        "\n",
        "\n",
        "train_pairs = pairs\n",
        "print(test_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZaEUtRRcCOa"
      },
      "outputs": [],
      "source": [
        "input_seq_class.highest_length = input_seq_class.highest_length + 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYorAtjwRMjv"
      },
      "source": [
        "# **Training Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB6aKt5Ed7Mk"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "epoch = 2\n",
        "in_size = input_seq_class.size_of_index\n",
        "out_size= out_seq_class.size_of_index\n",
        "encoder_ = Encoder(in_size, hidden_size).to(device)\n",
        "decoder_ = attention_decoder(hidden_size, out_size, input_seq_class.highest_length).to(device)\n",
        "#decoder_ = Decoder(hidden_size, out_size).to(device)\n",
        "train_iters(train_pairs, input_seq_class.highest_length, encoder_, decoder_)\n",
        "torch.save(encoder_, 'drive/MyDrive/Honour_Thesis/encoder.pth')\n",
        "torch.save(decoder_, 'drive/MyDrive/Honour_Thesis/decoder.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download( \"encoder.pth\" ) \n",
        "files.download( \"decoder.pth\" ) "
      ],
      "metadata": {
        "id": "bmK1YqPShu2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6CmIJusRXWa"
      },
      "source": [
        "# **Evaluation Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCrgFWyO4m3p"
      },
      "outputs": [],
      "source": [
        "def evaluate(testing_seq, encoder, decoder, output_class, max_length):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = testing_seq\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for i in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[i],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[i] = encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[0]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for i in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[i] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == 1:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(topi.item())\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:i + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfUj-Kge3gnl"
      },
      "outputs": [],
      "source": [
        "print(out_seq_class.element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bHQP1CP2tHT"
      },
      "outputs": [],
      "source": [
        "def evaluate_iter(pairs, encoder, decoder, out_seq_class, input_seq_class):\n",
        "    total_correct_prediction = 0\n",
        "    total_test_apirs = len(pairs)\n",
        "    for i in range(len(pairs)):\n",
        "        pair = pairs[i][0]\n",
        "        input_tensor = torch.tensor(pair[0], dtype=torch.long, device=device).view(-1, 1)\n",
        "        target, attentions = evaluate(input_tensor, encoder_, decoder_, out_seq_class, input_seq_class.highest_length)\n",
        "\n",
        "        true_target = pair[1]\n",
        "\n",
        "        print(true_target)\n",
        "\n",
        "    #return total_correct_prediction/total_test_apirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dCMKk895v3G"
      },
      "outputs": [],
      "source": [
        "def evaluate_iter(pairs, encoder, decoder, out_seq_class, input_seq_class):\n",
        "    total_correct_prediction = 0\n",
        "    total_test_apirs = len(pairs)\n",
        "    pair = pairs[100][0]\n",
        "    input_tensor = torch.tensor(pair[0], dtype=torch.long, device=device).view(-1, 1)\n",
        "    target, attentions = evaluate(input_tensor, encoder_, decoder_, out_seq_class, input_seq_class.highest_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs_ozGxz3Fce"
      },
      "outputs": [],
      "source": [
        "evaluate_iter(test_pairs, encoder_, decoder_, out_seq_class, input_seq_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attension graph**"
      ],
      "metadata": {
        "id": "xnx_fAazHRjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder_, attention_decoder, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)"
      ],
      "metadata": {
        "id": "FRDwVG5PHRAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateAndShowAttention()"
      ],
      "metadata": {
        "id": "CTbkt7EgL2cw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Training_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOH7HeTJbg6l27F6bINA0Qa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}